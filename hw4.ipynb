{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Homework 4: Transfer learning with the inception network\n",
    "## DUE Friday 17 November at 4pm (both sections)\n",
    "Submit your homework by publishing a notebook that cleanly displays your code, results, and plots to pdf or html\n",
    "\n",
    "## Description\n",
    "\n",
    "In this homework, we will fine-tune the existing [inception network](https://arxiv.org/pdf/1512.00567.pdf) to classify some new categories of images. Building and fitting the network from scratch is expensive and far beyond the scope of this assignment, so you will load the tensorflow graph and variables which have been pre-trained on the imagenet dataset. You are responsible for completing the skeleton code in this notebook in order to use tensorboard to investigate the graph architecture, gain insight into the behavior of the pre-trained network on a new image classification task, fine-tune the network for this new task (retraining the last layer), and finally evaluate the performance of the fine-tuned model.\n",
    "\n",
    "## Downloading the network and data\n",
    "\n",
    "In order to avoid clutter and extraneous technical details, we have created a supplementary module  `transfer_learning.py` that does most of the heavy lifting. We have only exposed the high-level which you are responsible for completing and understanding in full in this notebook.  While you are encouraged to read `transfer_learning.py`, you should not need to modify it in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import transfer_learning  # This module contains large amounts of pre-written code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will be using tensorboard to visualize the network, inputs, and other summaries of the training process. After running the following cell, start the server by executing ```tensorboard --logdir=./tmp/inception_v3_log``` from the current folder with the command line and navigate to http://localhost:6006/ in a new browser window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Ensure target log dir exists\n",
    "INCEPTION_LOG_DIR = './tmp/inception_v3_log'\n",
    "if not os.path.exists(INCEPTION_LOG_DIR):\n",
    "    os.makedirs(INCEPTION_LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Your goal in the assignment is to use a small collection of photos (500 to be exact) to construct a flower-image classifier capable of discerning differences between daisies, roses, dandelions, sunflowers, and tulips. This presents a obstacle to the use of state-of-the-art image classification models due to the heavily underdetermined nature of the problem (# features = #pixels * #channels >> #images). Moreover, training a large convolutional neural network from the ground up is can be computationally prohibitive. Alternatively, strongly biased linear classifiers like regularized logistic regression avoid these pitfalls. However, the performance of these models will be inadequate, because most important image features are non-linear transformations of the raw input and it is not obvious what these non-linear transformations should be.\n",
    "\n",
    "To circumvent these challenges, we will leverage what a pre-trained convolutional neural network has already learned about important image features from the [imagenet dataset](http://www.image-net.org/) by fine tuning it to our specific problem. To begin, download the the flower dataset [here](http://download.tensorflow.org/example_images/flower_photos.tgz) and extract the contents into a sub-folder of your working directory named ```data```. This dataset is from a [Google Code Lab](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), though note that we ask you to do considerably more than in that lab, so it is not helpful (nor are the commands covered therein valid answers to this homework).\n",
    "\n",
    "The following cell will partition the images into a training and testing set, then it will load lists of image paths for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'daisy'\n",
      "WARNING:tensorflow:WARNING: Folder daisy has more than 100 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'dandelion'\n",
      "WARNING:tensorflow:WARNING: Folder dandelion has more than 100 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'roses'\n",
      "WARNING:tensorflow:WARNING: Folder roses has more than 100 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'sunflowers'\n",
      "WARNING:tensorflow:WARNING: Folder sunflowers has more than 100 images. Some images will never be selected.\n",
      "INFO:tensorflow:Looking for images in 'tulips'\n",
      "WARNING:tensorflow:WARNING: Folder tulips has more than 100 images. Some images will never be selected.\n"
     ]
    }
   ],
   "source": [
    "# For performance reasons, we only use 100 images per class.\n",
    "training_images, testing_images, label_maps = transfer_learning.create_image_lists(\n",
    "    'C://Users/Think/Downloads/flower_photos', testing_percentage=10, max_number_images=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we will load the pre-trained inception model and look at its architecture in tensorboard. \n",
    "\n",
    "#### 1). Complete the following code block by opening a filewriter and pass it the graph object we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the inception model.\n",
    "# Warning! The first time you run this, it will try to download the inception model\n",
    "# from the internet. This is somewhat large (80MB), and can sometimes fail. If\n",
    "# it does, simply try again.\n",
    "\n",
    "graph, bottleneck, resized_input, softmax = transfer_learning.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'softmax:0' shape=(1, 1008) dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use a summary writer to write the loaded graph and display it in tensorboard.\n",
    "# Thens answer the questions below\n",
    "with graph.as_default():\n",
    "    jpeg_data, decoded_image = transfer_learning.make_jpeg_decoding()\n",
    "    file_writer = tf.summary.FileWriter(INCEPTION_LOG_DIR, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.) In tensorboard, expand the 'softmax' and 'logits' blocks in order to answer to following questions:\n",
    "* What are the dimensions of the inputs/outputs to/from the softmax block?\n",
    "* What are the shape and dimension of the input images?\n",
    "* When considered in isolation, what model is the 'softmax' block implementing? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Support your answers by including screenshots from tensorboard. (to insert images, simply copy and paste into the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write you answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifying out of the box\n",
    "\n",
    "Before retraining or modifying the network, we would like to better understand how it behaves on our dataset out of the box. Therefore, we will use it classify a couple of our flower images. First, we must create summary operations to write quanities of interest to tensorboard so that we may investigate. \n",
    "\n",
    "#### 3.) Complete the block below by implementing the following summaries:\n",
    "* `output_summary`: a summary of output probabilities (histogram of the output of the softmax block)\n",
    "* `input_summary`: a summary of image input to the network (image of resized and decoded jpeg data)\n",
    "* `bottleneck_summary`: a summary of bottleneck activations (histogram of the input to the softmax block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    tf.summary.histogram(\"output_summary\",softmax)\n",
    "    tf.summary.image(\"input_summary\",decoded_image)\n",
    "    tf.summary.histogram(\"bottleneck_summary\",bottleneck)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we will need a function that streamlines the process of performing inference given an image. This function will take as input the tensorflow session, a path to the image, and the ```summary_op``` we have created above. It will then open a ```t.summary.FileWriter``` and run the network forwards to obtain the formatted image, softmax output, and summaries. \n",
    "\n",
    "#### 4). Complete the code block below as instructed in each of the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classify_image(session, image_path, summary_op):\n",
    "    \"\"\"This functions reads a single image from disk and classifies\n",
    "    the image using the pre-trained network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session: the tensorflow session to use to run the operation\n",
    "    image_path: a string corresponding to the path of the image to load\n",
    "    summary_op: the summary operation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    label: an integer representing the label of the classified example.\n",
    "    softmax_output: the network's output multinomial probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open single image file and extract data\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_data = f.read()\n",
    "    ###----------student's code\n",
    "    \n",
    "    decoded_image_data = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n",
    "    softmax_output=session.run(softmax,feed_dict={resized_input:decoded_image_data})\n",
    "    bottleneck_output=session.run(bottleneck,feed_dict={resized_input:decoded_image_data})\n",
    "    summary=session.run(summary_op,feed_dict={softmax:softmax_output,decoded_image:decoded_image_data,bottleneck:bottleneck_output})\n",
    "    file_writer.add_summary(summary)\n",
    "    \n",
    "    # Return label\n",
    "    return(np.argmax(softmax_output),softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now ready to classify some images. Run the code in this cell in order to obtain the output label and navigate to tensorboard to look at the histogram and image summaries that have been logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'flower_photos', 'daisy', '100080576_f52e8ee070_n.jpg')\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        # We classify the image and print the label\n",
    "        imagenet_label,_ = classify_image(session, image_path, summary_op)\n",
    "print(imagenet_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.) Confirm that your code works by taking a screenshot of the input image in tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Include your screenshot here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you've completed the prevous steps correctly, you should see that the pre-trained network assigns the label 357 to our image. Cross referencing this with a [source](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57) that explains what each of the imagenet labels are, we see that our classifier right on target. The image was indeed a daisy, but this is solely a consequence of the fact that the class 'daisy' is included in the imagenet dataset. We should not expect such favorable outcome when this is not the case. We will test this assumtion by classifying a different type of flower image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'flower_photos', 'roses', '19440805164_920b28da61_n.jpg')\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        imagenet_label,softmax_output = classify_image(session, image_path, summary_op)\n",
    "print(imagenet_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 6.) Please explain what you have found by answering the following questions:\n",
    "\n",
    "* What does the network think this is an image of? Is this reasonable?\n",
    "* How confident is it in assigning this label (what is the probability it assigns)?\n",
    "* Why might the network behave this way?\n",
    "* What warning might you give about deep-learning/machine-learning in general based on these observations?\n",
    "\n",
    "Support your answers by referencing screenshots from tensorboard (of the input picture, softmax output, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Answer these questions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adapting The Network\n",
    "\n",
    "Clearly, this model will not suffice for our application out of the box. However, it is not unreasonable to think that we may be able to adapt what this network has \"learned\" about processing images for our purposes. Remember that the last layer of the network implements a well known classifier (see your answer to question 2). Under this interpretation, the layers of the network up until the last are responsible for learning a non-linear mapping between raw inputs (pixel values) and useful derived features. Consequently, we may remove the last layer, fix the weights of the remaining layers, use what remains as a black-box function transforming images into derived feature vectors, and finally fit a new classifier on the derived feature vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-Computing the bottlenecks\n",
    "\n",
    "To train the new classifier efficiently, we will want to pre-compute the derived feature vectors. We have been calling these the 'bottleneck', because they would limit the speed of training if we had to be re-computed them iteration. Additionally, its small size relative to the dimension of our raw input is essential in ensures that it provides a compact summary of the images for our classifier.\n",
    "\n",
    "#### 7). Which block in the tensorboard graph is the bottleneck?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write you answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have written code that will cache the derived features or 'bottleneck activation' for each images. In order to use this, we require that you complete a simple function that returns these activations.\n",
    "\n",
    "#### 8). Complete the code block below and compute the bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_bottleneck(session, image_data):\n",
    "    \"\"\"Computes the bottleneck for a given image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session: the tensorflow sessi\n",
    "    \n",
    "    on to use for the computation.\n",
    "    jpeg_data_tensor: the tensor to feed the jpeg data into.\n",
    "    bottleneck_tensor: the tensor representing.\n",
    "    image_data: a byte sequence representing the encoded jpeg data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array containing the bottleneck information for the image.\n",
    "    \"\"\"\n",
    "    #-- To be completed by the student\n",
    "    decoded_image_data = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n",
    "    bottleneck_output=session.run(bottleneck,feed_dict={resized_input:decoded_image_data})\n",
    "    return(np.array(bottleneck_output))\n",
    "    #--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Think\\\\Desktop\\\\python\\\\flower_photos\\\\daisy\\\\100080576_f52e8ee070_n.jpg'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'flower_photos', 'daisy', '100080576_f52e8ee070_n.jpg')\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1/444 bottlenecks\n",
      "Saved 21/444 bottlenecks\n",
      "Saved 41/444 bottlenecks\n",
      "Saved 61/444 bottlenecks\n",
      "Saved 81/444 bottlenecks\n",
      "Saved 101/444 bottlenecks\n",
      "Saved 121/444 bottlenecks\n",
      "Saved 141/444 bottlenecks\n",
      "Saved 161/444 bottlenecks\n",
      "Saved 181/444 bottlenecks\n",
      "Saved 201/444 bottlenecks\n",
      "Saved 221/444 bottlenecks\n",
      "Saved 241/444 bottlenecks\n",
      "Saved 261/444 bottlenecks\n",
      "Saved 281/444 bottlenecks\n",
      "Saved 301/444 bottlenecks\n",
      "Saved 321/444 bottlenecks\n",
      "Saved 341/444 bottlenecks\n",
      "Saved 361/444 bottlenecks\n",
      "Saved 381/444 bottlenecks\n",
      "Saved 401/444 bottlenecks\n",
      "Saved 421/444 bottlenecks\n",
      "Saved 441/444 bottlenecks\n",
      "Done computing bottlenecks!\n"
     ]
    }
   ],
   "source": [
    "# This cell generates all the bottlenecks. Warning: it may take a while\n",
    "# The results are cached so you only need to do it once -- if you change\n",
    "# your compute_bottleneck function, you will need to delete the existing\n",
    "# files to force the notebook to regenerate the bottlenecks (they are\n",
    "# found in ./data/bottlenecks)\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        transfer_learning.cache_bottlenecks(compute_bottleneck, session, training_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that these have been saved, we can load them in a format that is ammenable to use by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This loads the training data as a matrix of training examples\n",
    "# and a vector of labels\n",
    "training_data_set = transfer_learning.create_training_dataset(training_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are finally ready to train the new classifier. This is equivalent to replacing & retraining the final layer of our original network with a different number of output classes, so we will still be using tensorflow.\n",
    "\n",
    "#### 9). Complete the following function as instructed in the comments by:\n",
    "* Creating a new dense layer of weights, biases, and logits.\n",
    "* Computing/summarizing the cross-entropy loss.\n",
    "* Creating an optimizer/training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_final_layers(bottleneck_tensor, num_classes):\n",
    "    \"\"\"Create the operations for the last layer of the network to be retrained.\n",
    "    \n",
    "    This function should implement a logistic regression layer from the bottleneck\n",
    "    to the labels, trained using gradient descent. We have created the inputs\n",
    "    bottleneck_input and label_input for you. You are responsible for implementing\n",
    "    the logistic regression layer itself (as you have seen in lecture),\n",
    "    the predicted_output (predicted probability for each class), the cross entropy\n",
    "    loss, and the optimization and subsequent training step operation (an\n",
    "    operation that executes one step of gradient descent).  Note: be careful to\n",
    "    use the numerically stable implementation of cross entropy loss as discussed\n",
    "    in lecture.\n",
    "    \n",
    "    You should also record a summary for the loss you compute.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bottleneck_tensor: the bottleneck tensor in the original network\n",
    "    num_classes: the number of output classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bottleneck_input: the input placeholder for the bottleneck values\n",
    "    label_input: the input placeholder for the label values\n",
    "    logits: the tensor representing the unnormalized log probabilities\n",
    "        for each class.\n",
    "    train_step: an operation representing one gradient descent step.\n",
    "    \"\"\"\n",
    "    bottleneck_tensor_size = int(bottleneck.shape[1])\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        # This is the input for the bottleneck. It is created\n",
    "        # as a placeholder with default. During training, we will\n",
    "        # be passing in the bottlenecks, but during evaluation,\n",
    "        # the value will be propagated from the bottleneck computed\n",
    "        # from the image using the full network.\n",
    "        bottleneck_input = tf.placeholder_with_default(\n",
    "            bottleneck_tensor,\n",
    "            [None, bottleneck_tensor_size],\n",
    "            'bottleneck_input')\n",
    "        \n",
    "        # This is the input for the label (integer, 1 to number of classes)\n",
    "        label_input = tf.placeholder(tf.int64, [None], name='label_input')\n",
    "    \n",
    "    \n",
    "    # -- Start student must write\n",
    "    # Define weights, biases, and logit transforms\n",
    "    logits = tf.layers.dense(bottleneck_input, num_classes)\n",
    "    # Compute the cross entropy loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=label_input, logits=logits)\n",
    "    # Create a summary for the loss\n",
    "    loss_summary = tf.summary.scalar('cross_entropy', loss)\n",
    "    # Create a Gradient Descent Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    # Obtain a function which performs a single training step\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    # -- End student must write\n",
    "    \n",
    "    return bottleneck_input, label_input, logits, train_step, loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will also write a function to evaluate the model's classification accuracy.\n",
    "\n",
    "#### 10). Complete the following function as instructed in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(labels, logits):\n",
    "    \"\"\"Compute the accuracy for the predicted output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: The input labels (in a one-hot encoded fashion).\n",
    "    predicted_output: The predicted class probability for each output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the accuracy.\n",
    "    \"\"\"\n",
    "    #-- To be written by student\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_summary=tf.summary.scalar('accuracy', accuracy)\n",
    "    #--\n",
    "    \n",
    "    return accuracy, accuracy_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These functions have been completed for you. Make sure you understand what they are doing as it will help you figure out what to write in the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We create the necessary operations to fine tune the model.\n",
    "\n",
    "with graph.as_default():\n",
    "    bottleneck_input, label_input, logits, train_step, loss_summary = make_final_layers(bottleneck, len(label_maps))\n",
    "    accuracy, accuracy_summary = compute_accuracy(label_input, logits)\n",
    "    summary_op = tf.summary.merge([loss_summary, accuracy_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def execute_train_step(session: tf.Session, summary_writer: tf.summary.FileWriter, current_step: int):\n",
    "    \"\"\"This function runs a single training step.\n",
    "    \n",
    "    You may wish to print some progress information as you go along.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session: the tensorflow session to use to run the training step.\n",
    "    summary_writer: the summary file writer to write your summaries to.\n",
    "    current_step: the current step count (starting from zero)\n",
    "    \"\"\"\n",
    "    _, ac, summary = session.run((train_step, accuracy, summary_op),\n",
    "                       feed_dict={bottleneck_input: training_data_set['bottlenecks'],\n",
    "                                  label_input: training_data_set['labels']\n",
    "                                 })\n",
    "    \n",
    "    summary_writer.add_summary(summary, current_step)\n",
    "    \n",
    "    if current_step % 10 == 0:\n",
    "        print('Accuracy at step {0} is {1}'.format(current_step, ac))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_images(session: tf.Session, images_jpeg_data: [bytes], labels: [int]):\n",
    "    \"\"\"This function will evaluate the accuracy of our model on the specified data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session: the tensorflow session to use to run the evaluation step.\n",
    "    images_jpeg_data: a list of strings, with each element in the list corresponding\n",
    "        to the jpeg-encoded data for a given image\n",
    "    labels: a list of integers, with each element in the list corresponding to the label\n",
    "        of a given image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    This function should return the accuracy as a floating point number between\n",
    "    0 and 1 (proportion of correctly classified instances).\n",
    "    \"\"\"\n",
    "    correct = []\n",
    "    \n",
    "    for label, jpeg in zip(labels, images_jpeg_data):\n",
    "        image_data = session.run(decoded_image, feed_dict={jpeg_data: jpeg})\n",
    "        ac = session.run(accuracy, feed_dict={resized_input: image_data, label_input: [label]})\n",
    "        correct.append(ac)\n",
    "    \n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we put these functions together to train the final layer. Run the cell and confirm that your functions are working correctly (the accuracy on the training set should be increasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Starting training ----------------\n",
      "Accuracy at step 0 is 0.18918919563293457\n",
      "Accuracy at step 10 is 0.6756756901741028\n",
      "Accuracy at step 20 is 0.8738738894462585\n",
      "Accuracy at step 30 is 0.9459459185600281\n",
      "Accuracy at step 40 is 0.9909909963607788\n",
      "Accuracy at step 50 is 0.9954954981803894\n",
      "Accuracy at step 60 is 1.0\n",
      "Accuracy at step 70 is 1.0\n",
      "Accuracy at step 80 is 1.0\n",
      "Accuracy at step 90 is 1.0\n",
      "------------- Training done! -------------------\n",
      "---------- Loading testing data ----------------\n",
      "----------- Evaluating on testing --------------\n",
      "Evaluation accuracy was: 0.7857142686843872\n"
     ]
    }
   ],
   "source": [
    "# Let's run the training and evaluation!\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        print('------------- Starting training ----------------')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'retrained'), graph)\n",
    "        for i in range(100):\n",
    "            execute_train_step(session, summary_writer, i)\n",
    "        summary_writer.close()  \n",
    "        print('------------- Training done! -------------------')\n",
    "        print('---------- Loading testing data ----------------')\n",
    "        tlabels, timages = transfer_learning.get_testing_data(testing_images)\n",
    "        print('----------- Evaluating on testing --------------')\n",
    "        \n",
    "        eval_accuracy = evaluate_images(session, timages, tlabels)\n",
    "        print('Evaluation accuracy was: {0}'.format(eval_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 11). Navigate to tensorboard and use what you find to answer the follow questions:\n",
    "* What is the accuracy on the test set? How does this compare to random guessing?\n",
    "* How do the training loss and accuracy progress with the number training iterations?\n",
    "* What is the final loss/training error? What might this make you suspicious of? Can you provide a mathematical explanation for why this is happening?\n",
    "* What might you change in order to increase the classification performance of this model?\n",
    "\n",
    "As always, support your with screenshots from tensorboard and references to specific quantitative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
