{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "reference: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_val=images_train[:5000,:,:,:]\n",
    "y_onehot_val=labels_train[:5000,:]\n",
    "X_train=images_train[5000:,:,:,:]\n",
    "y_onehot=labels_train[5000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance on some data \n",
    "def perf_eval(logit_pred, y_true):\n",
    "    \"\"\"a function to evaluate performance of predicted y values vs true class labels\"\"\"\n",
    "    # now look at some data\n",
    "    print('    sample pred: {0}\\n    sample true: {1}'.format(np.argmax(logit_pred[0:20],1),np.argmax(y_true[0:20],1)))\n",
    "    # avg accuracy\n",
    "    is_correct_vals = np.equal(np.argmax(logit_pred,1),np.argmax(y_true,1))\n",
    "    #accuracy_vals = np.mean(is_correct_vals)\n",
    "    #print('    mean classification accuracy: {0}%'.format(100*accuracy_vals))\n",
    "    # Dig in a little deeper.  Where did we make correct predictions?  Does this seem reasonable?\n",
    "    print('    correct predictions by class: {0}'.format(y_true[is_correct_vals,:].sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cnn conv stuff\n",
    "def conv(x, W):\n",
    "    \"\"\"simple wrapper for tf.nn.conv2d\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    \"\"\"simple wrapper for tf.nn.max_pool with stride size 2\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def norm(x): \n",
    "    \"\"\"simple wrapper for tf.nn.lrn... See section 3.3 of Krizhevsky 2012 for details\"\"\"\n",
    "    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# elaborate the compute_logits code to include a variety of models\n",
    "def compute_logits(x, model_type, pkeep):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    if model_type=='lr':\n",
    "        W = tf.get_variable('W', shape=[32*32*3, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        logits = tf.add(tf.matmul(x, W), b, name='logits_lr')\n",
    "    elif model_type=='cnn_cf':\n",
    "        # try a 1 layer cnn\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # fc layer to logits\n",
    "        h_conv1_flat = tf.reshape(h_conv1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_conv1_flat, W_fc1), b_fc1, name='logits_cnn_cf')\n",
    "    elif model_type=='cnn_cnf':\n",
    "        # try a 1 layer cnn with a normalization layer\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # norm layer 1\n",
    "        h_norm1 = norm(h_conv1)\n",
    "        # fc layer to logits\n",
    "        h_flat = tf.reshape(h_norm1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_flat, W_fc1), b_fc1, name='logits_cnn_cnf')     \n",
    "    elif model_type=='cnn_cpncpnff':\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1, W_fc2), b_fc2, name='logits_cnn_cpncpnff')\n",
    "    elif model_type=='cnn_cpncpnfdf':\n",
    "        # same as above but add dropout.\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # insert a dropout layer here.\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='logits_cnn_cpncpnfdf')\n",
    "    elif model_type=='vgg16':\n",
    "        x = tf.reshape(x, [-1,32,32,3])\n",
    "        #layer1\n",
    "        #cnn layer 1.1\n",
    "        W_conv11 = tf.get_variable('W_conv11', shape=[3, 3, 3, 64])\n",
    "        b_conv11 = tf.get_variable('b_conv11', shape=[64])\n",
    "        print(W_conv11,b_conv11)\n",
    "        h_conv11 = tf.nn.relu(tf.add(conv(x, W_conv11), b_conv11))  #[batch,32,32,64]\n",
    "        print(h_conv11)\n",
    "        #cnn layer 1.2\n",
    "        W_conv12 = tf.get_variable('W_conv12', shape=[3, 3, 64, 64])\n",
    "        b_conv12 = tf.get_variable('b_conv12', shape=[64])\n",
    "        h_conv12 = tf.nn.relu(tf.add(conv(h_conv11, W_conv12), b_conv12))\n",
    "        #pool 1\n",
    "        pool1=maxpool(h_conv12)\n",
    "\n",
    "        #cnn layer 2.1\n",
    "        W_conv21 = tf.get_variable('W_conv21', shape=[3, 3, 64, 128])\n",
    "        b_conv21 = tf.get_variable('b_conv21', shape=[128])\n",
    "        h_conv21 = tf.nn.relu(tf.add(conv(pool1, W_conv21), b_conv21))  \n",
    "        #cnn layer 2.2\n",
    "        W_conv22 = tf.get_variable('W_conv22', shape=[3, 3, 128, 128])\n",
    "        b_conv22 = tf.get_variable('b_conv22', shape=[128])\n",
    "        h_conv22 = tf.nn.relu(tf.add(conv(h_conv21, W_conv22), b_conv22))\n",
    "        #pool 2\n",
    "        pool2=maxpool(h_conv22)\n",
    "\n",
    "        #cnn layer 3.1\n",
    "        W_conv31 = tf.get_variable('W_conv31', shape=[3, 3, 128, 256])\n",
    "        b_conv31 = tf.get_variable('b_conv31', shape=[256])\n",
    "        h_conv31 = tf.nn.relu(tf.add(conv(pool2, W_conv31), b_conv31))  \n",
    "        #cnn layer 3.2\n",
    "        W_conv32 = tf.get_variable('W_conv32', shape=[3, 3, 256, 256])\n",
    "        b_conv32 = tf.get_variable('b_conv32', shape=[256])\n",
    "        h_conv32 = tf.nn.relu(tf.add(conv(h_conv31, W_conv32), b_conv32))\n",
    "        #cnn layer 3.3\n",
    "        W_conv33 = tf.get_variable('W_conv33', shape=[3, 3, 256, 256])\n",
    "        b_conv33 = tf.get_variable('b_conv33', shape=[256])\n",
    "        h_conv33 = tf.nn.relu(tf.add(conv(h_conv32, W_conv33), b_conv33))\n",
    "        #pool 3\n",
    "        pool3=maxpool(h_conv33)\n",
    "\n",
    "        #cnn layer 4.1\n",
    "        W_conv41 = tf.get_variable('W_conv41', shape=[3, 3, 256, 512])\n",
    "        b_conv41 = tf.get_variable('b_conv41', shape=[512])\n",
    "        h_conv41 = tf.nn.relu(tf.add(conv(pool3, W_conv41), b_conv41))  \n",
    "        #cnn layer 4.2\n",
    "        W_conv42 = tf.get_variable('W_conv42', shape=[3, 3, 512, 512])\n",
    "        b_conv42 = tf.get_variable('b_conv42', shape=[512])\n",
    "        h_conv42 = tf.nn.relu(tf.add(conv(h_conv41, W_conv42), b_conv42))\n",
    "        #cnn layer 4.3\n",
    "        W_conv43 = tf.get_variable('W_conv43', shape=[3, 3, 512, 512])\n",
    "        b_conv43 = tf.get_variable('b_conv43', shape=[512])\n",
    "        h_conv43 = tf.nn.relu(tf.add(conv(h_conv42, W_conv43), b_conv43))\n",
    "        #pool4\n",
    "        pool4=maxpool(h_conv43)\n",
    "\n",
    "        #cnn layer 5.1\n",
    "        W_conv51 = tf.get_variable('W_conv51', shape=[3, 3, 512, 512])\n",
    "        b_conv51 = tf.get_variable('b_conv51', shape=[512])\n",
    "        h_conv51 = tf.nn.relu(tf.add(conv(pool4, W_conv51), b_conv51))  \n",
    "        #cnn layer 5.2\n",
    "        W_conv52 = tf.get_variable('W_conv52', shape=[3, 3, 512, 512])\n",
    "        b_conv52 = tf.get_variable('b_conv52', shape=[512])\n",
    "        h_conv52 = tf.nn.relu(tf.add(conv(h_conv51, W_conv52), b_conv52))\n",
    "        #cnn layer 5.3\n",
    "        W_conv53 = tf.get_variable('W_conv53', shape=[3, 3, 512, 512])\n",
    "        b_conv53 = tf.get_variable('b_conv53', shape=[512])\n",
    "        h_conv53 = tf.nn.relu(tf.add(conv(h_conv52, W_conv53), b_conv53))\n",
    "        #pool 5\n",
    "        pool5=maxpool(h_conv53)\n",
    "\n",
    "        #fc1-4096\n",
    "        shape=int(np.prod(pool5.get_shape()[1:]))\n",
    "        pool5_flat=tf.reshape(pool5,[-1,shape])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[shape,4096])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[4096])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(pool5_flat, W_fc1), b_fc1))\n",
    "\n",
    "        #fc2-4096\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[4096,4096])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[4096])\n",
    "        h_fc2 = tf.nn.relu(tf.add(tf.matmul(h_fc1, W_fc2), b_fc2))\n",
    "\n",
    "        #fc3-1000\n",
    "        W_fc3 = tf.get_variable('W_fc3', shape=[4096,1000])\n",
    "        b_fc3 = tf.get_variable('b_fc3', shape=[1000])\n",
    "        h_fc3 = tf.nn.relu(tf.add(tf.matmul(h_fc2, W_fc3), b_fc3))\n",
    "        \n",
    "        #from softmax to logits\n",
    "        W_sf = tf.get_variable('W_sf', shape=[1000,10])\n",
    "        b_sf = tf.get_variable('b_sf', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc3, W_sf), b_sf, name='logits_vgg16')\n",
    "    else: \n",
    "        print('error not a valid model type')\n",
    "    print(logits)\n",
    "    return logits\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    numerical_instability_example = 0\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        print(logits,y)\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# choose case to run \n",
    "opt_method = 'sgd'\n",
    "model_type =  'vgg16'\n",
    "dir_name = 'logs/scratch04x/'\n",
    "batch_size = 50\n",
    "n=50000\n",
    "n_val=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W_conv11:0' shape=(3, 3, 3, 64) dtype=float32_ref> <tf.Variable 'b_conv11:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"model/Relu:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"model/logits_vgg16:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"model/logits_vgg16:0\", shape=(?, 10), dtype=float32) Tensor(\"y:0\", shape=(?, 10), dtype=float32)\n",
      "Step   0: training accuracy 0.1000\n",
      "    sample pred: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.  100.    0.    0.    0.    0.    0.    0.    0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if opt_method == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        elif opt_method == 'rms':\n",
    "            opt = tf.train.RMSPropOptimizer(.001)\n",
    "        elif opt_method == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(301):\n",
    "            batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)\n",
    "            X_batch = X_train[batch,:,:,:].reshape([batch_size,-1])\n",
    "            y_batch = y_onehot[batch]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%100==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%100 == 0:\n",
    "                X_batch = X_train[0:1000,:,:,:].reshape([1000,-1])\n",
    "                y_batch = y_onehot[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                # further diagnostics\n",
    "                perf_eval(train_logits, y_batch)\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                X_batch = X_val.reshape([n_val,-1])\n",
    "                y_batch = y_onehot_val\n",
    "                (val_error, summary) = sess.run((accuracy,summary_op), {x:X_batch, y:y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_error), flush=True)\n",
    "                summary_writer_val.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/logits_lr:0\", shape=(?, 10), dtype=float32) Tensor(\"y:0\", shape=(?, 10), dtype=float32)\n",
      "Step   0: training accuracy 0.1210\n",
      "    sample pred: [0 0 0 0 0 4 0 4 0 0 0 0 0 0 0 0 0 4 0 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 100.    0.    0.    0.   21.    0.    0.    0.    0.    0.]\n",
      "Step   0: val accuracy 0.1242\n",
      "Step 100: training accuracy 0.2060\n",
      "    sample pred: [7 7 1 5 5 5 5 5 9 9 0 9 1 1 1 7 5 5 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  52.   0.   0.   0.  58.   5.  72.   0.  18.]\n",
      "Step 100: val accuracy 0.2030\n",
      "Step 200: training accuracy 0.2200\n",
      "    sample pred: [7 0 0 5 5 5 5 5 8 7 0 7 0 1 1 0 0 5 7 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 88.   6.   0.   0.   0.  57.  12.  55.   2.   0.]\n",
      "Step 200: val accuracy 0.2176\n",
      "Step 300: training accuracy 0.1640\n",
      "    sample pred: [4 3 1 1 3 3 3 3 3 1 0 3 3 1 1 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 22.  46.   0.  84.  10.   0.   2.   0.   0.   0.]\n",
      "Step 300: val accuracy 0.1882\n"
     ]
    }
   ],
   "source": [
    "model_type =  'lr'\n",
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if opt_method == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        elif opt_method == 'rms':\n",
    "            opt = tf.train.RMSPropOptimizer(.001)\n",
    "        elif opt_method == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(301):\n",
    "            batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)\n",
    "            X_batch = X_train[batch,:,:,:].reshape([batch_size,-1])\n",
    "            y_batch = y_onehot[batch]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%100==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%100 == 0:\n",
    "                X_batch = X_train[0:1000,:,:,:].reshape([1000,-1])\n",
    "                y_batch = y_onehot[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                # further diagnostics\n",
    "                perf_eval(train_logits, y_batch)\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                X_batch = X_val.reshape([n_val,-1])\n",
    "                y_batch = y_onehot_val\n",
    "                (val_error, summary) = sess.run((accuracy,summary_op), {x:X_batch, y:y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_error), flush=True)\n",
    "                summary_writer_val.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: training accuracy 0.1150\n",
      "    sample pred: [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.    0.    0.    0.    0.    0.    0.    0.  115.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if opt_method == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        elif opt_method == 'rms':\n",
    "            opt = tf.train.RMSPropOptimizer(.001)\n",
    "        elif opt_method == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(20001):\n",
    "            batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)\n",
    "            X_batch = X_train[batch,:,:,:].reshape([batch_size,-1])\n",
    "            y_batch = y_onehot[batch]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%100==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%100 == 0:\n",
    "                X_batch = X_train[0:1000,:,:,:].reshape([1000,-1])\n",
    "                y_batch = y_onehot[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                # further diagnostics\n",
    "                perf_eval(train_logits, y_batch)\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                X_batch = X_val.reshape([n_val,-1])\n",
    "                y_batch = y_onehot_val\n",
    "                (val_error, summary) = sess.run((accuracy,summary_op), {x:X_batch, y:y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_error), flush=True)\n",
    "                summary_writer_val.add_summary(summary, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
