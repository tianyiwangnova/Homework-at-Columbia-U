{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Use PrettyTensor to simplify Neural Network construction.\n",
    "import prettytensor as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar10 import img_size, num_channels, num_classes\n",
    "img_size_cropped = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_image(image, training):\n",
    "    # This function takes a single image as input,\n",
    "    # and a boolean whether to build the training or testing graph.\n",
    "    \n",
    "    if training:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Randomly crop the input image.\n",
    "        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "\n",
    "        # Randomly flip the image horizontally.\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        # Some of these functions may overflow and result in pixel\n",
    "        # values beyond the [0, 1] range. It is unclear from the\n",
    "        # documentation of TensorFlow 0.10.0rc0 whether this is\n",
    "        # intended. A simple solution is to limit the range.\n",
    "\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Crop the input image around the centre so it is the same\n",
    "        # size as images that are randomly cropped during training.\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=img_size_cropped,\n",
    "                                                       target_width=img_size_cropped)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(images, training):\n",
    "    # Use TensorFlow to loop over all the input images and call\n",
    "    # the function above which takes a single image as input.\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c3827101d56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdistorted_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "distorted_images = pre_process(images=x, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'map/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 28, 28, 3) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distorted_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch():\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d8b105cd8eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_batch' is not defined"
     ]
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes from TA's tensorflow tutorial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start with our existing model code\n",
    "\n",
    "def compute_logits(x):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    W = tf.get_variable('W', shape=[32*32*3, 10])\n",
    "    b = tf.get_variable('b', shape=[10])\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Note: this function is implemented in tensorflow as\n",
    "# tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# We have included it here for illustration only, please don't use it.\n",
    "def compute_cross_entropy(logits, y):\n",
    "    y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: accuracy is 0.09080000221729279\n",
      "Step 11: accuracy is 0.10100000351667404\n",
      "Step 21: accuracy is 0.10199999809265137\n",
      "Step 31: accuracy is 0.10040000081062317\n",
      "Step 41: accuracy is 0.09960000216960907\n",
      "Step 51: accuracy is 0.09759999811649323\n",
      "Step 61: accuracy is 0.10000000149011612\n",
      "Step 71: accuracy is 0.10939999669790268\n",
      "Step 81: accuracy is 0.09600000083446503\n",
      "Step 91: accuracy is 0.09839999675750732\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    logits = compute_logits(x)\n",
    "    loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train_step = opt.minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Let's put the summaries below\n",
    "    \n",
    "    # create summary for loss and accuracy\n",
    "    tf.summary.scalar('loss', loss) \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # create summary for logits\n",
    "    #tf.summary.histogram('logits', logits)\n",
    "    \n",
    "    # create summary for input image\n",
    "    tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter('logs/example1', sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(100):\n",
    "            x_batch, y_true_batch = random_batch()\n",
    "            x_batch=x_batch.reshape(5000,32*32*3)\n",
    "            y_true_batch=y_true_batch.reshape(5000,10)\n",
    "            feed_dict_train = {x: x_batch,y: y_true_batch}\n",
    "            \n",
    "            _, ac, summary = sess.run((train_step, accuracy, summary_op),\n",
    "                                      feed_dict=feed_dict_train)\n",
    "            # write the summary output to file\n",
    "            \n",
    "            summary_writer.add_summary(summary, i)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Step {0}: accuracy is {1}'.format(i + 1, ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use the Teacher's code (lec04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val=images_train[:5000,:,:,:]\n",
    "y_onehot_val=labels_train[:5000,:]\n",
    "X_train=images_train[5000:,:,:,:]\n",
    "y_onehot=labels_train[5000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance on some data \n",
    "def perf_eval(logit_pred, y_true):\n",
    "    \"\"\"a function to evaluate performance of predicted y values vs true class labels\"\"\"\n",
    "    # now look at some data\n",
    "    print('    sample pred: {0}\\n    sample true: {1}'.format(np.argmax(logit_pred[0:20],1),np.argmax(y_true[0:20],1)))\n",
    "    # avg accuracy\n",
    "    is_correct_vals = np.equal(np.argmax(logit_pred,1),np.argmax(y_true,1))\n",
    "    #accuracy_vals = np.mean(is_correct_vals)\n",
    "    #print('    mean classification accuracy: {0}%'.format(100*accuracy_vals))\n",
    "    # Dig in a little deeper.  Where did we make correct predictions?  Does this seem reasonable?\n",
    "    print('    correct predictions by class: {0}'.format(y_true[is_correct_vals,:].sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn conv stuff\n",
    "def conv(x, W):\n",
    "    \"\"\"simple wrapper for tf.nn.conv2d\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    \"\"\"simple wrapper for tf.nn.max_pool with stride size 2\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def norm(x): \n",
    "    \"\"\"simple wrapper for tf.nn.lrn... See section 3.3 of Krizhevsky 2012 for details\"\"\"\n",
    "    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elaborate the compute_logits code to include a variety of models\n",
    "def compute_logits(x, model_type, pkeep):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    if model_type=='lr':\n",
    "        W = tf.get_variable('W', shape=[32*32*3, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        logits = tf.add(tf.matmul(x, W), b, name='logits_lr')\n",
    "    elif model_type=='cnn_cf':\n",
    "        # try a 1 layer cnn\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # fc layer to logits\n",
    "        h_conv1_flat = tf.reshape(h_conv1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_conv1_flat, W_fc1), b_fc1, name='logits_cnn_cf')\n",
    "    elif model_type=='cnn_cnf':\n",
    "        # try a 1 layer cnn with a normalization layer\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # norm layer 1\n",
    "        h_norm1 = norm(h_conv1)\n",
    "        # fc layer to logits\n",
    "        h_flat = tf.reshape(h_norm1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_flat, W_fc1), b_fc1, name='logits_cnn_cnf')     \n",
    "    elif model_type=='cnn_cpncpnff':\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1, W_fc2), b_fc2, name='logits_cnn_cpncpnff')\n",
    "    elif model_type=='cnn_cpncpnfdf':\n",
    "        # same as above but add dropout.\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # insert a dropout layer here.\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='logits_cnn_cpncpnfdf')\n",
    "    else: \n",
    "        print('error not a valid model type')\n",
    "\n",
    "    return logits\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "    cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose case to run \n",
    "opt_method = 'sgd'\n",
    "model_type = 'lr' \n",
    "dir_name = 'logs/scratch04x/'\n",
    "batch_size = 100\n",
    "n=50000\n",
    "n_val=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_batch = X_train[batch,:,:,:]..reshape([batch_size,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3072)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: training accuracy 0.1000\n",
      "    sample pred: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.  100.    0.    0.    0.    0.    0.    0.    0.]\n",
      "Step   0: val accuracy 0.1038\n",
      "Step 100: training accuracy 0.1600\n",
      "    sample pred: [1 1 1 1 5 1 1 1 1 1 0 1 1 1 1 0 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 40.  72.   0.   0.   0.  24.   0.   5.  19.   0.]\n",
      "Step 100: val accuracy 0.1696\n",
      "Step 200: training accuracy 0.1840\n",
      "    sample pred: [0 0 0 1 5 1 0 1 0 1 0 1 0 1 1 0 0 2 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 99.  42.  30.   0.   0.   7.   6.   0.   0.   0.]\n",
      "Step 200: val accuracy 0.1798\n",
      "Step 300: training accuracy 0.2000\n",
      "    sample pred: [2 0 1 5 5 5 5 5 5 5 0 5 5 1 1 2 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 43.  21.  46.   0.   0.  75.   0.   0.   0.  15.]\n",
      "Step 300: val accuracy 0.2146\n",
      "Step 400: training accuracy 0.2480\n",
      "    sample pred: [6 0 1 1 5 5 5 5 8 8 0 1 5 1 1 0 0 5 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 64.  38.   0.   0.   0.  65.  27.   0.  54.   0.]\n",
      "Step 400: val accuracy 0.2650\n",
      "Step 500: training accuracy 0.2340\n",
      "    sample pred: [6 3 9 3 3 6 3 5 8 6 0 9 5 9 9 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 40.   0.   0.  22.   0.  53.  64.   0.  26.  29.]\n",
      "Step 500: val accuracy 0.2424\n",
      "Step 600: training accuracy 0.1700\n",
      "    sample pred: [9 9 9 9 5 9 9 9 9 9 0 9 9 9 9 9 9 7 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   6.    0.   33.    3.    0.    8.    0.    6.    0.  114.]\n",
      "Step 600: val accuracy 0.1616\n",
      "Step 700: training accuracy 0.2460\n",
      "    sample pred: [2 3 9 3 3 3 3 3 8 9 8 9 3 9 9 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 44.  17.  16.  72.   0.   0.   0.   0.  44.  53.]\n",
      "Step 700: val accuracy 0.2540\n",
      "Step 800: training accuracy 0.2070\n",
      "    sample pred: [6 9 9 1 6 6 6 6 9 9 0 9 6 1 9 6 6 6 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  38.   0.   0.   0.   2.  94.   0.   0.  72.]\n",
      "Step 800: val accuracy 0.1958\n",
      "Step 900: training accuracy 0.2780\n",
      "    sample pred: [6 8 1 1 6 6 6 6 8 9 8 9 1 1 1 8 8 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  47.   0.   0.  44.   0.  65.   0.  86.  36.]\n",
      "Step 900: val accuracy 0.2826\n",
      "Step 1000: training accuracy 0.1930\n",
      "    sample pred: [7 7 1 1 7 7 7 7 7 7 8 7 7 1 1 7 2 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.   54.   21.    0.    0.    1.    0.  102.   14.    1.]\n",
      "Step 1000: val accuracy 0.1858\n",
      "Step 1100: training accuracy 0.1560\n",
      "    sample pred: [6 3 9 3 5 5 3 3 9 3 0 3 3 9 9 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.   2.   0.  71.   3.  38.  14.   0.   0.  27.]\n",
      "Step 1100: val accuracy 0.1682\n",
      "Step 1200: training accuracy 0.2340\n",
      "    sample pred: [7 8 1 3 3 3 3 3 8 8 8 3 3 9 1 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  20.   0.  68.   0.   0.   0.  56.  78.  12.]\n",
      "Step 1200: val accuracy 0.2428\n",
      "Step 1300: training accuracy 0.2210\n",
      "    sample pred: [7 2 9 2 5 2 2 5 2 7 8 3 2 2 9 2 2 5 3 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   0.  90.  12.   8.  21.   0.  50.  33.   7.]\n",
      "Step 1300: val accuracy 0.2100\n",
      "Step 1400: training accuracy 0.2730\n",
      "    sample pred: [7 9 1 1 5 5 5 5 1 9 8 9 1 1 1 5 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  71.   0.   3.   0.  53.  24.  42.  29.  51.]\n",
      "Step 1400: val accuracy 0.2688\n",
      "Step 1500: training accuracy 0.2740\n",
      "    sample pred: [2 3 9 3 5 2 3 3 9 9 8 9 3 9 9 2 3 5 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  10.  65.  46.   0.  20.   4.   5.  47.  77.]\n",
      "Step 1500: val accuracy 0.2726\n",
      "Step 1600: training accuracy 0.2650\n",
      "    sample pred: [7 8 1 1 6 6 4 6 8 8 8 9 6 1 1 4 8 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  3.  30.   1.   0.  52.   0.  68.  15.  91.   5.]\n",
      "Step 1600: val accuracy 0.2742\n",
      "Step 1700: training accuracy 0.2750\n",
      "    sample pred: [7 9 9 1 3 1 7 1 9 9 8 9 7 9 1 7 8 7 9 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  6.  41.   0.   2.   0.   0.   1.  86.  68.  71.]\n",
      "Step 1700: val accuracy 0.2586\n",
      "Step 1800: training accuracy 0.2970\n",
      "    sample pred: [6 4 1 1 6 6 4 6 8 1 0 4 4 1 1 0 0 4 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 41.  43.   0.   2.  63.   8.  66.   0.  73.   1.]\n",
      "Step 1800: val accuracy 0.3044\n",
      "Step 1900: training accuracy 0.2120\n",
      "    sample pred: [7 5 7 5 5 5 5 5 5 3 8 3 5 5 3 5 3 5 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   0.   0.  24.  54.  58.  35.  11.  30.   0.]\n",
      "Step 1900: val accuracy 0.2226\n",
      "Step 2000: training accuracy 0.2060\n",
      "    sample pred: [7 8 0 8 8 8 7 8 8 8 0 8 0 8 8 0 0 7 8 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 47.   0.   0.   0.   1.   0.   0.  70.  88.   0.]\n",
      "Step 2000: val accuracy 0.1966\n",
      "Step 2100: training accuracy 0.2390\n",
      "    sample pred: [7 2 1 1 3 4 2 4 1 1 0 1 2 1 1 2 2 4 1 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 31.  47.  66.  19.  40.   0.   0.  29.   7.   0.]\n",
      "Step 2100: val accuracy 0.2496\n",
      "Step 2200: training accuracy 0.1930\n",
      "    sample pred: [7 4 1 1 4 4 4 4 1 1 8 1 4 1 1 4 4 4 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  55.   1.   0.  80.   0.   0.  25.  32.   0.]\n",
      "Step 2200: val accuracy 0.2140\n",
      "Step 2300: training accuracy 0.2530\n",
      "    sample pred: [7 0 1 1 3 1 3 3 1 1 8 9 3 1 1 0 3 3 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 29.  58.   0.  51.   0.   0.   0.  50.  58.   7.]\n",
      "Step 2300: val accuracy 0.2596\n",
      "Step 2400: training accuracy 0.2610\n",
      "    sample pred: [6 9 9 3 3 6 3 3 9 9 0 9 3 9 9 0 3 3 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 51.   1.   0.  58.   0.   0.  59.   3.   1.  88.]\n",
      "Step 2400: val accuracy 0.2578\n",
      "Step 2500: training accuracy 0.2050\n",
      "    sample pred: [7 0 0 3 3 6 3 3 0 3 0 3 0 9 0 0 0 3 3 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 103.    7.    0.   32.    3.    0.   25.   21.    2.   12.]\n",
      "Step 2500: val accuracy 0.2026\n",
      "Step 2600: training accuracy 0.2810\n",
      "    sample pred: [7 9 9 8 5 5 5 5 8 9 8 9 5 9 9 5 8 5 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  16.   0.   8.   0.  69.   0.  41.  83.  64.]\n",
      "Step 2600: val accuracy 0.2626\n",
      "Step 2700: training accuracy 0.3060\n",
      "    sample pred: [7 7 1 1 6 4 4 4 8 1 8 9 1 1 1 7 1 7 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  58.   0.   0.  55.   0.  48.  67.  70.   8.]\n",
      "Step 2700: val accuracy 0.3120\n",
      "Step 2800: training accuracy 0.2720\n",
      "    sample pred: [7 3 1 1 3 6 3 1 1 1 8 1 1 1 1 7 1 7 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  67.   0.  23.   0.  12.  56.  59.  54.   0.]\n",
      "Step 2800: val accuracy 0.2820\n",
      "Step 2900: training accuracy 0.1750\n",
      "    sample pred: [5 5 1 5 5 5 5 5 5 5 8 5 5 1 1 5 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.  18.   0.   0.  38.  85.   0.   0.  27.   5.]\n",
      "Step 2900: val accuracy 0.1856\n",
      "Step 3000: training accuracy 0.2120\n",
      "    sample pred: [2 2 1 1 6 6 2 6 2 1 0 1 2 1 1 2 2 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  62.  70.   3.   1.   0.  59.  16.   0.   0.]\n",
      "Step 3000: val accuracy 0.2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3100: training accuracy 0.2300\n",
      "    sample pred: [5 3 1 1 5 5 3 5 8 1 8 3 3 1 1 5 3 5 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  4.  51.   0.  29.  34.  55.   0.   1.  55.   1.]\n",
      "Step 3100: val accuracy 0.2598\n",
      "Step 3200: training accuracy 0.2460\n",
      "    sample pred: [7 9 1 1 5 6 5 5 1 1 8 9 5 1 1 5 1 5 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  67.   0.   0.  17.  62.  48.  33.   0.  19.]\n",
      "Step 3200: val accuracy 0.2562\n",
      "Step 3300: training accuracy 0.3380\n",
      "    sample pred: [2 9 9 1 3 6 3 3 8 9 8 9 2 9 1 0 3 3 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 45.  33.  54.  31.   0.   0.  55.   0.  53.  67.]\n",
      "Step 3300: val accuracy 0.3384\n",
      "Step 3400: training accuracy 0.2810\n",
      "    sample pred: [7 3 9 3 3 4 3 3 8 9 8 9 3 9 9 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   5.   6.  72.  30.  10.   2.  36.  55.  65.]\n",
      "Step 3400: val accuracy 0.2838\n",
      "Step 3500: training accuracy 0.2590\n",
      "    sample pred: [7 8 1 1 5 5 7 5 8 9 8 9 7 9 1 7 8 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  4.  35.   0.   0.   0.  14.   0.  91.  82.  33.]\n",
      "Step 3500: val accuracy 0.2458\n",
      "Step 3600: training accuracy 0.3000\n",
      "    sample pred: [7 9 9 9 3 4 2 4 9 9 8 9 9 9 9 2 2 7 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 15.   2.  63.  14.  26.   0.   0.  21.  61.  98.]\n",
      "Step 3600: val accuracy 0.2862\n",
      "Step 3700: training accuracy 0.2580\n",
      "    sample pred: [7 5 9 5 5 5 5 5 2 9 8 9 5 9 9 5 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   0.  30.   0.   0.  80.   0.  57.  33.  58.]\n",
      "Step 3700: val accuracy 0.2426\n",
      "Step 3800: training accuracy 0.2700\n",
      "    sample pred: [7 8 1 1 5 4 4 8 8 1 8 8 1 1 1 8 8 6 1 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  50.   0.   0.  47.   7.  47.  29.  90.   0.]\n",
      "Step 3800: val accuracy 0.2778\n",
      "Step 3900: training accuracy 0.2250\n",
      "    sample pred: [5 0 1 1 5 5 5 5 1 1 0 1 1 1 1 0 1 5 1 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 47.  62.  11.   0.   0.  59.   0.   2.  44.   0.]\n",
      "Step 3900: val accuracy 0.2440\n",
      "Step 4000: training accuracy 0.2320\n",
      "    sample pred: [7 9 9 9 9 9 9 8 9 9 8 9 9 9 9 9 9 7 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.   10.    0.    0.    0.    0.    4.   57.   55.  106.]\n",
      "Step 4000: val accuracy 0.2080\n",
      "Step 4100: training accuracy 0.2990\n",
      "    sample pred: [2 9 1 1 3 2 2 3 9 9 8 9 2 9 1 2 3 3 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 10.  38.  70.  31.   0.   4.   0.   1.  74.  71.]\n",
      "Step 4100: val accuracy 0.2930\n",
      "Step 4200: training accuracy 0.2350\n",
      "    sample pred: [4 9 1 1 5 5 5 5 1 1 8 9 1 1 1 4 1 5 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  64.   0.   0.  63.  56.   0.   0.   0.  52.]\n",
      "Step 4200: val accuracy 0.2374\n",
      "Step 4300: training accuracy 0.2580\n",
      "    sample pred: [2 0 1 1 5 5 5 5 2 1 0 5 5 1 1 0 0 5 5 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 85.  28.  40.   1.   0.  60.  20.  19.   5.   0.]\n",
      "Step 4300: val accuracy 0.2708\n",
      "Step 4400: training accuracy 0.2460\n",
      "    sample pred: [5 0 9 5 5 5 3 5 8 9 0 3 3 9 9 0 3 5 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 84.   1.  10.  44.   3.  48.   1.   0.  15.  40.]\n",
      "Step 4400: val accuracy 0.2462\n",
      "Step 4500: training accuracy 0.1990\n",
      "    sample pred: [5 9 9 9 5 5 5 5 9 9 0 9 9 9 9 5 3 5 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   1.    0.    0.   13.   12.   54.   11.    0.    0.  108.]\n",
      "Step 4500: val accuracy 0.1964\n",
      "Step 4600: training accuracy 0.1960\n",
      "    sample pred: [5 0 0 5 5 5 5 5 8 5 0 5 5 5 0 0 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 44.   0.   0.   6.  38.  84.  15.   0.   9.   0.]\n",
      "Step 4600: val accuracy 0.2068\n",
      "Step 4700: training accuracy 0.3090\n",
      "    sample pred: [7 0 9 5 5 5 5 5 7 9 0 9 5 9 1 0 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 71.  19.   6.   2.   0.  74.  29.  73.   0.  35.]\n",
      "Step 4700: val accuracy 0.3002\n",
      "Step 4800: training accuracy 0.2300\n",
      "    sample pred: [4 9 9 1 4 4 4 4 9 9 8 9 9 9 1 4 1 4 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  51.   0.   0.  73.   0.  12.   2.   0.  92.]\n",
      "Step 4800: val accuracy 0.2196\n",
      "Step 4900: training accuracy 0.2310\n",
      "    sample pred: [7 9 9 9 9 8 9 8 9 9 8 9 9 9 9 8 8 7 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.   0.   0.  10.   0.   0.  14.  36.  73.  96.]\n",
      "Step 4900: val accuracy 0.2154\n",
      "Step 5000: training accuracy 0.2150\n",
      "    sample pred: [7 8 8 8 3 4 4 4 8 8 8 8 4 8 8 4 3 4 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   0.   0.  25.  77.   0.   8.  13.  92.   0.]\n",
      "Step 5000: val accuracy 0.2248\n",
      "Step 5100: training accuracy 0.2730\n",
      "    sample pred: [7 0 1 1 7 4 7 1 1 1 0 9 7 1 1 0 0 7 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 78.  59.   0.   0.  21.   0.   0.  87.   3.  25.]\n",
      "Step 5100: val accuracy 0.2530\n",
      "Step 5200: training accuracy 0.1870\n",
      "    sample pred: [6 6 9 6 6 6 6 6 6 9 0 6 6 9 9 0 6 6 6 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 56.   0.   0.   0.   0.   0.  95.   4.   1.  31.]\n",
      "Step 5200: val accuracy 0.1878\n",
      "Step 5300: training accuracy 0.1850\n",
      "    sample pred: [7 0 9 0 0 0 0 0 9 9 0 9 0 9 9 0 0 7 9 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 104.    3.    0.    0.    0.    0.   11.   25.    1.   41.]\n",
      "Step 5300: val accuracy 0.1700\n",
      "Step 5400: training accuracy 0.2100\n",
      "    sample pred: [6 6 1 1 5 6 6 3 6 6 8 6 6 1 1 6 3 6 3 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  22.   0.  14.   4.  28.  86.  13.  33.  10.]\n",
      "Step 5400: val accuracy 0.2320\n",
      "Step 5500: training accuracy 0.2590\n",
      "    sample pred: [7 9 1 1 5 5 5 5 1 9 8 9 1 9 1 1 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  59.  10.   0.   0.  55.   0.  10.  53.  71.]\n",
      "Step 5500: val accuracy 0.2508\n",
      "Step 5600: training accuracy 0.3380\n",
      "    sample pred: [7 0 1 1 3 6 3 3 1 9 0 9 3 9 1 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 71.  42.   0.  60.  11.   0.  49.  69.   0.  36.]\n",
      "Step 5600: val accuracy 0.3296\n",
      "Step 5700: training accuracy 0.3140\n",
      "    sample pred: [2 0 9 1 5 5 2 5 2 9 0 9 2 9 9 0 2 5 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 69.  21.  73.   6.   0.  54.  19.   0.   1.  71.]\n",
      "Step 5700: val accuracy 0.2832\n",
      "Step 5800: training accuracy 0.2240\n",
      "    sample pred: [7 9 9 9 5 5 5 5 9 9 0 9 9 9 9 9 9 5 9 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  14.    7.    5.    0.    1.   56.   18.   18.    0.  105.]\n",
      "Step 5800: val accuracy 0.2276\n",
      "Step 5900: training accuracy 0.2430\n",
      "    sample pred: [2 0 1 1 5 1 5 1 1 1 8 1 1 1 1 0 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 43.  75.  24.   0.  23.  36.   0.  12.  30.   0.]\n",
      "Step 5900: val accuracy 0.2580\n",
      "Step 6000: training accuracy 0.2880\n",
      "    sample pred: [4 3 1 1 3 4 3 3 2 9 0 9 3 9 1 0 3 3 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 40.  48.   7.  55.  62.   0.   0.   0.  39.  37.]\n",
      "Step 6000: val accuracy 0.3088\n",
      "Step 6100: training accuracy 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sample pred: [7 2 1 1 3 4 4 4 2 7 8 7 4 1 1 4 2 4 4 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  22.  41.   7.  76.   4.   2.  24.  18.   6.]\n",
      "Step 6100: val accuracy 0.1984\n",
      "Step 6200: training accuracy 0.2610\n",
      "    sample pred: [7 0 0 5 5 5 5 5 8 8 0 5 5 5 0 0 0 5 5 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 75.   0.   0.   4.   5.  66.  13.  47.  51.   0.]\n",
      "Step 6200: val accuracy 0.2600\n",
      "Step 6300: training accuracy 0.3070\n",
      "    sample pred: [7 2 9 8 5 2 2 8 2 9 0 9 2 9 9 0 2 7 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 31.   2.  88.   0.   6.  14.   0.  52.  56.  58.]\n",
      "Step 6300: val accuracy 0.2838\n",
      "Step 6400: training accuracy 0.2530\n",
      "    sample pred: [7 0 0 8 3 4 7 4 7 7 0 7 0 0 0 0 0 7 7 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 83.   1.  11.   2.  35.   0.   4.  75.  42.   0.]\n",
      "Step 6400: val accuracy 0.2542\n",
      "Step 6500: training accuracy 0.2790\n",
      "    sample pred: [7 9 9 9 2 4 2 2 2 9 8 9 2 9 9 2 2 7 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  8.   0.  86.   0.  10.   0.  19.  20.  48.  88.]\n",
      "Step 6500: val accuracy 0.2602\n",
      "Step 6600: training accuracy 0.3100\n",
      "    sample pred: [7 9 9 9 3 4 4 5 9 9 8 9 9 9 9 2 2 5 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.   54.    3.   44.   31.   29.   34.   14.  101.]\n",
      "Step 6600: val accuracy 0.2894\n",
      "Step 6700: training accuracy 0.2420\n",
      "    sample pred: [7 7 7 1 3 4 7 3 7 9 8 9 7 9 9 7 2 7 7 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.   14.   37.   16.   12.    0.   17.  102.    4.   40.]\n",
      "Step 6700: val accuracy 0.2130\n",
      "Step 6800: training accuracy 0.1840\n",
      "    sample pred: [2 1 1 1 5 1 2 1 1 1 0 1 1 1 1 2 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.  76.  59.   4.   0.  21.   9.   0.   0.  13.]\n",
      "Step 6800: val accuracy 0.1878\n",
      "Step 6900: training accuracy 0.1670\n",
      "    sample pred: [5 5 9 5 5 5 5 5 9 9 8 5 5 9 9 5 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   2.   4.   0.   0.  93.  12.   0.   8.  48.]\n",
      "Step 6900: val accuracy 0.1716\n",
      "Step 7000: training accuracy 0.2690\n",
      "    sample pred: [7 5 9 5 5 5 5 5 9 9 0 9 5 9 9 5 5 5 9 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 26.   0.   0.   0.   8.  91.   0.  28.  45.  71.]\n",
      "Step 7000: val accuracy 0.2648\n",
      "Step 7100: training accuracy 0.1890\n",
      "    sample pred: [5 5 1 1 5 5 5 5 1 1 8 5 5 1 1 5 1 5 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  53.   2.   8.   0.  75.   0.   1.  50.   0.]\n",
      "Step 7100: val accuracy 0.2114\n",
      "Step 7200: training accuracy 0.2050\n",
      "    sample pred: [7 8 8 8 3 8 3 8 8 8 8 8 8 9 8 0 8 3 8 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 12.   1.   5.  38.   0.   1.  24.  16.  98.  10.]\n",
      "Step 7200: val accuracy 0.2120\n",
      "Step 7300: training accuracy 0.2620\n",
      "    sample pred: [7 0 1 1 6 6 6 6 1 1 0 9 0 1 1 0 1 6 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 99.  56.   0.   3.   0.   0.  71.  23.   1.   9.]\n",
      "Step 7300: val accuracy 0.2584\n",
      "Step 7400: training accuracy 0.2300\n",
      "    sample pred: [7 5 0 5 5 5 5 5 2 5 0 5 5 1 1 0 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 49.  20.  43.   1.   3.  75.   1.  37.   1.   0.]\n",
      "Step 7400: val accuracy 0.2410\n",
      "Step 7500: training accuracy 0.2530\n",
      "    sample pred: [7 7 7 1 3 4 7 7 7 7 0 7 7 9 1 7 0 7 7 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  35.   35.    0.    5.    6.    0.   19.  105.   38.   10.]\n",
      "Step 7500: val accuracy 0.2386\n",
      "Step 7600: training accuracy 0.2040\n",
      "    sample pred: [7 7 1 1 7 4 7 4 1 1 8 7 7 1 1 7 1 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  67.   0.   0.  37.   0.   0.  89.   8.   3.]\n",
      "Step 7600: val accuracy 0.2056\n",
      "Step 7700: training accuracy 0.1470\n",
      "    sample pred: [4 4 1 1 5 4 4 4 4 1 0 4 4 1 1 4 4 4 4 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 17.  18.   0.   4.  92.  13.   1.   0.   2.   0.]\n",
      "Step 7700: val accuracy 0.1644\n",
      "Step 7800: training accuracy 0.3390\n",
      "    sample pred: [7 0 9 1 3 4 3 3 9 9 0 9 3 9 9 0 3 5 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 84.  25.   1.  42.  37.  29.   5.  42.   1.  73.]\n",
      "Step 7800: val accuracy 0.3142\n",
      "Step 7900: training accuracy 0.3020\n",
      "    sample pred: [2 9 9 1 5 5 5 5 9 9 0 9 9 9 9 0 5 5 9 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  51.   28.   21.    0.    1.   61.    0.   18.   21.  101.]\n",
      "Step 7900: val accuracy 0.2792\n",
      "Step 8000: training accuracy 0.3400\n",
      "    sample pred: [7 9 9 1 6 6 2 4 9 9 0 9 2 9 9 0 2 6 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 37.  29.  64.   0.  29.   0.  56.  29.   3.  93.]\n",
      "Step 8000: val accuracy 0.3142\n",
      "Step 8100: training accuracy 0.2260\n",
      "    sample pred: [2 2 9 2 6 6 2 2 2 9 0 9 2 9 9 2 2 2 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  4.   2.  91.   1.   0.   0.  36.  14.   1.  77.]\n",
      "Step 8100: val accuracy 0.2094\n",
      "Step 8200: training accuracy 0.3130\n",
      "    sample pred: [7 9 9 1 5 4 4 4 9 9 8 9 4 9 1 4 6 4 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  48.   4.   0.  72.  21.  54.  26.  28.  60.]\n",
      "Step 8200: val accuracy 0.3052\n",
      "Step 8300: training accuracy 0.2150\n",
      "    sample pred: [6 8 8 8 3 6 3 8 8 8 8 8 3 9 1 8 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  19.   0.  47.  12.   0.  33.   0.  96.   8.]\n",
      "Step 8300: val accuracy 0.2322\n",
      "Step 8400: training accuracy 0.2560\n",
      "    sample pred: [7 0 9 1 7 7 7 7 7 9 0 7 7 9 1 0 0 7 7 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  64.   42.    0.    0.    0.    0.    0.  103.   12.   35.]\n",
      "Step 8400: val accuracy 0.2316\n",
      "Step 8500: training accuracy 0.3120\n",
      "    sample pred: [7 0 1 1 5 6 7 5 7 1 0 7 7 1 1 0 0 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 70.  50.   0.   0.   0.  18.  36.  91.  47.   0.]\n",
      "Step 8500: val accuracy 0.2930\n",
      "Step 8600: training accuracy 0.2700\n",
      "    sample pred: [2 2 1 1 3 6 3 3 2 8 8 3 2 2 1 2 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  5.  21.  76.  43.   0.   0.  34.  22.  68.   1.]\n",
      "Step 8600: val accuracy 0.2748\n",
      "Step 8700: training accuracy 0.2620\n",
      "    sample pred: [7 3 9 3 3 6 3 3 3 9 8 3 3 9 9 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 15.   0.   0.  74.   3.   0.  25.  53.  65.  27.]\n",
      "Step 8700: val accuracy 0.2804\n",
      "Step 8800: training accuracy 0.3120\n",
      "    sample pred: [4 9 9 1 3 4 4 4 9 9 0 9 9 9 9 0 4 4 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 60.  27.   0.  19.  70.   0.  29.   1.  12.  94.]\n",
      "Step 8800: val accuracy 0.2988\n",
      "Step 8900: training accuracy 0.3040\n",
      "    sample pred: [7 0 9 9 6 4 4 4 9 9 0 9 4 9 9 0 4 4 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 68.   7.   0.   0.  76.   0.  21.  37.   4.  91.]\n",
      "Step 8900: val accuracy 0.2832\n",
      "Step 9000: training accuracy 0.3290\n",
      "    sample pred: [2 9 1 1 6 6 6 6 9 9 8 9 6 9 1 2 6 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  9.  42.  45.   0.   0.   0.  83.  24.  49.  77.]\n",
      "Step 9000: val accuracy 0.3092\n",
      "Step 9100: training accuracy 0.3500\n",
      "    sample pred: [7 0 1 1 5 6 3 5 7 1 0 5 5 1 1 0 5 7 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 60.  43.   9.  15.   0.  56.  47.  77.  43.   0.]\n",
      "Step 9100: val accuracy 0.3426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9200: training accuracy 0.2830\n",
      "    sample pred: [7 7 9 9 3 6 7 4 7 9 0 9 7 9 9 7 3 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.   0.  14.   9.  23.  11.  54.  98.   1.  72.]\n",
      "Step 9200: val accuracy 0.2542\n",
      "Step 9300: training accuracy 0.2450\n",
      "    sample pred: [7 3 1 1 3 6 3 3 1 1 8 1 1 1 1 3 1 3 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.  67.   2.  42.   0.   0.  59.  26.  47.   0.]\n",
      "Step 9300: val accuracy 0.2584\n",
      "Step 9400: training accuracy 0.3490\n",
      "    sample pred: [7 9 9 9 6 4 6 4 9 9 8 9 9 9 9 4 4 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  5.  10.   0.   0.  48.   3.  71.  57.  63.  92.]\n",
      "Step 9400: val accuracy 0.3220\n",
      "Step 9500: training accuracy 0.2320\n",
      "    sample pred: [2 2 8 8 5 4 2 4 2 8 8 8 2 2 8 0 2 5 4 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 15.   0.  65.   0.  40.  31.   0.   0.  80.   1.]\n",
      "Step 9500: val accuracy 0.2508\n",
      "Step 9600: training accuracy 0.2660\n",
      "    sample pred: [6 3 1 1 3 6 3 3 1 1 0 3 3 1 1 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 52.  51.   8.  73.   0.   0.  50.   0.  15.  17.]\n",
      "Step 9600: val accuracy 0.2664\n",
      "Step 9700: training accuracy 0.3490\n",
      "    sample pred: [7 9 9 1 6 6 6 6 9 9 0 9 7 9 9 0 6 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 44.  30.   1.   0.  34.   0.  69.  80.   1.  90.]\n",
      "Step 9700: val accuracy 0.3042\n",
      "Step 9800: training accuracy 0.2500\n",
      "    sample pred: [7 1 1 1 5 1 7 1 1 1 0 9 1 1 1 7 1 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 22.  73.   4.   7.   0.  37.   1.  86.   7.  13.]\n",
      "Step 9800: val accuracy 0.2408\n",
      "Step 9900: training accuracy 0.3160\n",
      "    sample pred: [7 9 9 5 5 5 5 5 9 9 8 9 9 9 9 7 5 5 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.    0.    5.   42.   56.   26.   65.   21.  101.]\n",
      "Step 9900: val accuracy 0.3018\n",
      "Step 10000: training accuracy 0.2370\n",
      "    sample pred: [2 0 1 1 3 4 3 3 1 1 0 1 1 1 1 0 1 3 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 84.  60.  26.  40.  26.   0.   1.   0.   0.   0.]\n",
      "Step 10000: val accuracy 0.2554\n",
      "Step 10100: training accuracy 0.3150\n",
      "    sample pred: [7 0 9 1 3 4 4 4 9 9 0 9 4 9 9 0 4 4 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 69.  21.   0.  14.  79.   6.   0.  27.  26.  73.]\n",
      "Step 10100: val accuracy 0.3072\n",
      "Step 10200: training accuracy 0.2650\n",
      "    sample pred: [6 8 1 1 5 6 3 8 1 1 8 8 1 1 1 8 1 5 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  56.   0.  13.   5.  27.  64.  14.  86.   0.]\n",
      "Step 10200: val accuracy 0.2838\n",
      "Step 10300: training accuracy 0.2240\n",
      "    sample pred: [2 9 9 9 5 5 5 5 9 9 8 9 9 9 9 0 9 5 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  11.    0.   29.    4.    8.   51.    0.    0.   13.  108.]\n",
      "Step 10300: val accuracy 0.2218\n",
      "Step 10400: training accuracy 0.3740\n",
      "    sample pred: [7 0 1 1 5 4 7 5 1 9 0 9 7 9 1 0 0 7 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 70.  41.   0.   0.  20.  39.  40.  83.  55.  26.]\n",
      "Step 10400: val accuracy 0.3594\n",
      "Step 10500: training accuracy 0.2170\n",
      "    sample pred: [2 8 8 8 3 2 2 8 2 8 8 8 2 9 8 2 2 2 1 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  12.  75.   9.   0.   1.   0.  23.  92.   5.]\n",
      "Step 10500: val accuracy 0.2186\n",
      "Step 10600: training accuracy 0.3160\n",
      "    sample pred: [6 9 9 1 6 6 6 6 2 9 8 9 6 9 9 2 6 6 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  31.  48.   0.   3.   0.  81.   9.  71.  73.]\n",
      "Step 10600: val accuracy 0.3040\n",
      "Step 10700: training accuracy 0.2670\n",
      "    sample pred: [7 8 1 1 6 4 4 8 1 1 8 8 1 1 1 8 1 4 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  53.   0.   0.  66.   0.  35.  27.  86.   0.]\n",
      "Step 10700: val accuracy 0.2728\n",
      "Step 10800: training accuracy 0.2480\n",
      "    sample pred: [7 0 1 1 3 1 3 3 1 1 0 3 0 1 1 0 3 3 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 93.  44.   0.  61.   0.   0.   3.   7.  31.   9.]\n",
      "Step 10800: val accuracy 0.2344\n",
      "Step 10900: training accuracy 0.2050\n",
      "    sample pred: [2 2 2 1 6 6 2 6 2 6 8 2 2 2 1 2 2 2 6 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  17.  89.   5.   0.   2.  47.  24.  21.   0.]\n",
      "Step 10900: val accuracy 0.2030\n",
      "Step 11000: training accuracy 0.2650\n",
      "    sample pred: [4 0 9 1 3 4 4 4 9 9 0 9 4 9 9 0 4 4 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 72.  12.   0.  21.  78.   0.   8.   0.   1.  73.]\n",
      "Step 11000: val accuracy 0.2642\n",
      "Step 11100: training accuracy 0.3400\n",
      "    sample pred: [7 9 9 1 5 6 6 5 1 9 0 9 9 9 1 7 1 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  49.  30.   1.   5.  37.  58.  66.   0.  93.]\n",
      "Step 11100: val accuracy 0.3030\n",
      "Step 11200: training accuracy 0.2700\n",
      "    sample pred: [7 8 1 1 4 4 4 4 1 1 8 8 4 1 1 8 4 7 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.  36.   0.   2.  69.   0.   0.  60.  94.   7.]\n",
      "Step 11200: val accuracy 0.2610\n",
      "Step 11300: training accuracy 0.2860\n",
      "    sample pred: [7 4 1 1 5 4 4 4 1 9 0 9 4 1 1 4 4 4 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 26.  61.   5.   0.  74.  11.  20.  34.  31.  24.]\n",
      "Step 11300: val accuracy 0.2888\n",
      "Step 11400: training accuracy 0.1750\n",
      "    sample pred: [5 5 1 5 5 5 5 5 3 5 8 5 5 5 1 5 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   6.   0.  15.   0.  88.   9.  10.  41.   6.]\n",
      "Step 11400: val accuracy 0.1708\n",
      "Step 11500: training accuracy 0.2500\n",
      "    sample pred: [4 4 0 8 3 4 3 3 2 8 0 3 4 9 1 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 38.   5.  17.  43.  76.   0.   0.   0.  68.   3.]\n",
      "Step 11500: val accuracy 0.2608\n",
      "Step 11600: training accuracy 0.2300\n",
      "    sample pred: [7 0 0 0 3 4 7 3 7 9 0 7 7 9 0 0 0 7 7 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 97.   0.   1.   9.  18.   0.   0.  84.   1.  20.]\n",
      "Step 11600: val accuracy 0.2108\n",
      "Step 11700: training accuracy 0.2500\n",
      "    sample pred: [5 9 9 5 5 5 5 5 9 9 8 9 5 9 9 5 5 5 9 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.   8.   0.   0.   1.  85.   7.  16.  42.  90.]\n",
      "Step 11700: val accuracy 0.2372\n",
      "Step 11800: training accuracy 0.2350\n",
      "    sample pred: [6 9 1 1 5 1 5 1 1 1 0 9 1 1 1 0 1 5 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 57.  81.   5.   0.   6.  30.  20.   0.   1.  35.]\n",
      "Step 11800: val accuracy 0.2340\n",
      "Step 11900: training accuracy 0.2580\n",
      "    sample pred: [7 8 1 1 5 4 4 4 1 1 0 1 1 1 1 4 1 4 1 1]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  8.  66.   1.   0.  71.  14.   0.  36.  60.   2.]\n",
      "Step 11900: val accuracy 0.2696\n",
      "Step 12000: training accuracy 0.2290\n",
      "    sample pred: [5 3 1 1 3 3 3 3 3 9 0 3 3 9 1 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 58.  31.   0.  81.  12.  15.   8.   0.   4.  20.]\n",
      "Step 12000: val accuracy 0.2472\n",
      "Step 12100: training accuracy 0.2300\n",
      "    sample pred: [2 2 9 1 3 2 2 2 2 9 8 9 2 9 9 2 2 2 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  19.  96.   4.   0.   0.   0.  15.  32.  64.]\n",
      "Step 12100: val accuracy 0.2174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12200: training accuracy 0.2940\n",
      "    sample pred: [7 9 9 1 6 6 6 6 9 9 8 9 7 9 9 7 9 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.   23.    1.    6.    0.    2.   70.   79.   11.  102.]\n",
      "Step 12200: val accuracy 0.2596\n",
      "Step 12300: training accuracy 0.3050\n",
      "    sample pred: [7 9 9 1 3 4 3 3 9 9 8 9 3 9 1 3 3 3 9 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  36.   0.  68.  46.  21.   9.  47.   0.  78.]\n",
      "Step 12300: val accuracy 0.2748\n",
      "Step 12400: training accuracy 0.2770\n",
      "    sample pred: [2 0 1 1 5 4 2 5 2 1 0 1 2 1 1 0 0 5 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 92.  43.  51.   3.  22.  38.   0.  19.   5.   4.]\n",
      "Step 12400: val accuracy 0.2796\n",
      "Step 12500: training accuracy 0.2120\n",
      "    sample pred: [6 3 8 3 3 4 3 3 3 3 8 3 3 9 3 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   0.   0.  72.  21.   0.  45.   3.  65.   6.]\n",
      "Step 12500: val accuracy 0.2280\n",
      "Step 12600: training accuracy 0.3040\n",
      "    sample pred: [2 9 9 1 3 2 3 3 2 9 0 9 2 9 1 2 3 3 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  5.  49.  73.  45.   0.   0.  33.  10.  14.  75.]\n",
      "Step 12600: val accuracy 0.2840\n",
      "Step 12700: training accuracy 0.2970\n",
      "    sample pred: [2 5 9 5 5 4 5 5 2 9 8 5 5 9 1 5 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 14.  23.  26.   0.  43.  74.  26.   0.  34.  57.]\n",
      "Step 12700: val accuracy 0.2990\n",
      "Step 12800: training accuracy 0.3150\n",
      "    sample pred: [7 4 1 1 5 4 6 6 4 7 0 6 6 1 1 0 6 7 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 59.  32.   0.   0.  37.  28.  81.  69.   9.   0.]\n",
      "Step 12800: val accuracy 0.2912\n",
      "Step 12900: training accuracy 0.3090\n",
      "    sample pred: [7 9 9 8 3 4 4 4 9 9 8 9 7 9 9 7 4 7 9 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   2.   4.  14.  62.   1.   0.  73.  87.  66.]\n",
      "Step 12900: val accuracy 0.2896\n",
      "Step 13000: training accuracy 0.2750\n",
      "    sample pred: [7 7 1 1 3 1 3 1 1 1 8 7 7 1 1 7 1 7 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  50.  26.  26.   1.   9.   1.  89.  73.   0.]\n",
      "Step 13000: val accuracy 0.2814\n",
      "Step 13100: training accuracy 0.2640\n",
      "    sample pred: [7 8 9 8 3 8 7 8 9 9 8 9 9 9 9 8 8 7 9 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.   2.   0.   4.   0.   0.  23.  68.  93.  74.]\n",
      "Step 13100: val accuracy 0.2422\n",
      "Step 13200: training accuracy 0.2720\n",
      "    sample pred: [7 0 0 1 6 4 6 4 6 6 0 6 6 6 1 0 6 6 6 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 76.  12.   0.   0.  53.   0.  81.  26.  23.   1.]\n",
      "Step 13200: val accuracy 0.2620\n",
      "Step 13300: training accuracy 0.3170\n",
      "    sample pred: [7 9 1 1 3 1 3 3 1 9 0 9 7 9 1 7 3 7 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.  54.  29.  37.   0.  17.  34.  78.   0.  67.]\n",
      "Step 13300: val accuracy 0.2990\n",
      "Step 13400: training accuracy 0.1980\n",
      "    sample pred: [7 3 0 3 3 3 3 3 3 3 0 3 3 9 0 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 83.   3.   8.  83.   0.   0.   0.   9.   2.  10.]\n",
      "Step 13400: val accuracy 0.1978\n",
      "Step 13500: training accuracy 0.2570\n",
      "    sample pred: [7 8 8 8 7 2 7 8 2 8 8 8 7 9 1 8 8 7 1 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  23.  37.   1.   1.   0.  13.  77.  93.  12.]\n",
      "Step 13500: val accuracy 0.2484\n",
      "Step 13600: training accuracy 0.2940\n",
      "    sample pred: [2 0 1 5 5 4 3 3 2 9 0 5 5 9 1 0 3 5 5 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 54.  20.  56.  27.  27.  48.   2.   0.  30.  30.]\n",
      "Step 13600: val accuracy 0.3034\n",
      "Step 13700: training accuracy 0.2560\n",
      "    sample pred: [6 9 1 1 3 1 3 3 1 9 8 9 3 9 1 3 1 3 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  59.   0.  56.   0.   5.  14.   2.  49.  71.]\n",
      "Step 13700: val accuracy 0.2514\n",
      "Step 13800: training accuracy 0.2690\n",
      "    sample pred: [2 9 9 1 3 4 2 4 9 9 8 9 9 9 9 2 2 2 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   1.   36.   70.    6.   19.    0.   17.    0.   20.  100.]\n",
      "Step 13800: val accuracy 0.2514\n",
      "Step 13900: training accuracy 0.3110\n",
      "    sample pred: [7 0 1 1 5 4 4 4 1 1 0 9 7 1 1 0 1 7 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 76.  58.   1.   1.  53.  10.   0.  79.  16.  17.]\n",
      "Step 13900: val accuracy 0.2922\n",
      "Step 14000: training accuracy 0.1790\n",
      "    sample pred: [7 3 1 1 3 3 3 3 1 1 8 3 3 1 1 3 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  32.  10.  85.   7.   0.   0.  30.  15.   0.]\n",
      "Step 14000: val accuracy 0.2000\n",
      "Step 14100: training accuracy 0.2170\n",
      "    sample pred: [6 8 8 8 5 6 8 8 8 8 8 8 8 8 8 8 8 5 8 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  1.   2.  24.   1.  10.  19.  63.   0.  97.   0.]\n",
      "Step 14100: val accuracy 0.2268\n",
      "Step 14200: training accuracy 0.2080\n",
      "    sample pred: [6 1 1 1 5 1 6 1 1 1 0 1 1 1 1 0 1 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 34.  79.   0.   1.   0.  22.  72.   0.   0.   0.]\n",
      "Step 14200: val accuracy 0.2144\n",
      "Step 14300: training accuracy 0.2990\n",
      "    sample pred: [5 0 9 1 3 4 3 3 1 9 0 9 3 9 1 0 3 3 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 61.  37.   2.  58.  39.  42.   0.   1.   0.  59.]\n",
      "Step 14300: val accuracy 0.2882\n",
      "Step 14400: training accuracy 0.3080\n",
      "    sample pred: [5 0 1 1 5 4 5 5 1 9 0 9 5 9 1 0 3 5 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 69.  38.   0.  24.  43.  56.   5.   0.  15.  58.]\n",
      "Step 14400: val accuracy 0.3138\n",
      "Step 14500: training accuracy 0.2040\n",
      "    sample pred: [7 9 9 9 3 9 9 8 9 9 8 9 9 9 9 9 9 7 9 9]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   0.    0.    0.   14.    0.    7.   12.   38.   19.  114.]\n",
      "Step 14500: val accuracy 0.1850\n",
      "Step 14600: training accuracy 0.3300\n",
      "    sample pred: [7 0 1 1 5 6 5 5 1 1 0 9 1 1 1 0 1 5 1 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 78.  52.   0.   0.   0.  50.  41.  38.  56.  15.]\n",
      "Step 14600: val accuracy 0.3334\n",
      "Step 14700: training accuracy 0.2570\n",
      "    sample pred: [5 0 0 5 5 5 5 5 2 5 0 5 5 5 0 0 0 5 3 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 75.   4.  21.  17.   0.  71.   1.  14.  54.   0.]\n",
      "Step 14700: val accuracy 0.2438\n",
      "Step 14800: training accuracy 0.2200\n",
      "    sample pred: [2 2 1 1 6 4 2 4 2 1 0 9 2 9 1 2 2 4 1 4]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  2.  42.  67.   2.  42.   0.  43.   0.   0.  22.]\n",
      "Step 14800: val accuracy 0.2322\n",
      "Step 14900: training accuracy 0.2720\n",
      "    sample pred: [6 9 1 1 6 4 6 4 1 1 0 9 1 9 1 0 1 6 1 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 26.  74.  14.   3.  42.   0.  70.  17.   0.  26.]\n",
      "Step 14900: val accuracy 0.2686\n",
      "Step 15000: training accuracy 0.3140\n",
      "    sample pred: [7 0 9 1 3 6 3 3 1 9 0 9 0 9 9 0 0 7 9 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 82.  34.   0.  12.   0.   0.  39.  65.   6.  76.]\n",
      "Step 15000: val accuracy 0.2890\n",
      "Step 15100: training accuracy 0.3580\n",
      "    sample pred: [7 9 9 1 6 6 6 6 9 9 8 9 9 9 9 7 6 7 9 6]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  22.  42.   0.   0.   0.  67.  70.  60.  97.]\n",
      "Step 15100: val accuracy 0.3176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15200: training accuracy 0.2930\n",
      "    sample pred: [7 0 9 1 5 5 5 5 1 9 0 9 5 9 1 0 5 5 5 5]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 65.  34.   6.   0.   1.  83.   0.  50.   1.  53.]\n",
      "Step 15200: val accuracy 0.2834\n",
      "Step 15300: training accuracy 0.3020\n",
      "    sample pred: [2 9 9 5 5 5 5 5 2 9 0 9 5 9 9 0 2 5 9 0]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 42.   0.  57.   0.   0.  63.   1.   0.  51.  88.]\n",
      "Step 15300: val accuracy 0.2870\n",
      "Step 15400: training accuracy 0.2870\n",
      "    sample pred: [2 2 9 1 5 4 2 2 2 9 8 9 2 9 1 2 2 7 9 2]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  46.  90.   1.   5.  14.   0.  41.  24.  66.]\n",
      "Step 15400: val accuracy 0.2556\n",
      "Step 15500: training accuracy 0.2890\n",
      "    sample pred: [7 0 1 1 3 2 3 3 2 7 0 7 3 1 1 0 3 7 1 3]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 90.  42.  48.  41.   0.   0.   6.  61.   1.   0.]\n",
      "Step 15500: val accuracy 0.2822\n",
      "Step 15600: training accuracy 0.2240\n",
      "    sample pred: [2 8 1 1 3 1 2 8 1 1 8 8 8 9 1 8 1 2 1 8]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [  0.  48.  46.   3.   5.   0.   0.  15.  91.  16.]\n",
      "Step 15600: val accuracy 0.2314\n",
      "Step 15700: training accuracy 0.3090\n",
      "    sample pred: [7 0 1 1 7 1 7 1 1 9 0 9 7 9 1 7 1 7 1 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [ 50.  62.   0.   0.   0.   1.  33.  92.  24.  47.]\n",
      "Step 15700: val accuracy 0.2758\n",
      "Step 15800: training accuracy 0.2350\n",
      "    sample pred: [7 7 7 1 3 6 7 6 2 7 0 7 7 9 1 7 2 7 7 7]\n",
      "    sample true: [6 7 9 0 5 2 3 3 3 9 0 9 2 9 1 0 2 3 9 6]\n",
      "    correct predictions by class: [   2.   29.   36.    6.    0.    0.   36.  104.   13.    9.]\n",
      "Step 15800: val accuracy 0.2254\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if opt_method == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        elif opt_method == 'rms':\n",
    "            opt = tf.train.RMSPropOptimizer(.001)\n",
    "        elif opt_method == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(20001):\n",
    "            batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)\n",
    "            X_batch = X_train[batch,:,:,:].reshape([batch_size,-1])\n",
    "            y_batch = y_onehot[batch]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%100==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%100 == 0:\n",
    "                X_batch = X_train[0:1000,:,:,:].reshape([1000,-1])\n",
    "                y_batch = y_onehot[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                # further diagnostics\n",
    "                perf_eval(train_logits, y_batch)\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                X_batch = X_val.reshape([n_val,-1])\n",
    "                y_batch = y_onehot_val\n",
    "                (val_error, summary) = sess.run((accuracy,summary_op), {x:X_batch, y:y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_error), flush=True)\n",
    "                summary_writer_val.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
